{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tool\n",
    "import pickle\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import textwrap\n",
    "\n",
    "## plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "## Bert\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "## module\n",
    "from module.ner_trainer import NRE_Trainer\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-chinese\",\n",
    "    num_labels = 3,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "NRE_trainer = NRE_Trainer(model, None, None)\n",
    "NRE_trainer.model.load_state_dict(torch.load('params/best-model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測名字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenized_sentence = tokenizer_chinese.encode(text)\n",
    "    input_ids = tokenized_sentence\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(text):\n",
    "    sentences = []\n",
    "    if len(text) < 500:\n",
    "        sentences.append(text)\n",
    "        \n",
    "    elif len(text) > 500 and len(text) <= 1000:\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    elif len(text) > 1000 and len(text) <= 1500:\n",
    "        mid = int(len(text)/2)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[mid-250:mid+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    elif len(text) > 1500 and len(text) <= 2000:\n",
    "        point_1 = int(len(text)*0.25)\n",
    "        point_2 = int(len(text)*0.75)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[point_1-250:point_1+250])\n",
    "        sentences.append(text[point_2-250:point_2+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    else:\n",
    "        mid = int(len(text)/2)\n",
    "        point_1 = int(len(text)*0.25)\n",
    "        point_2 = int(len(text)*0.75)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[point_1-250:point_1+250])\n",
    "        sentences.append(text[mid-250:mid+250])\n",
    "        sentences.append(text[point_2-250:point_2+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_sentence(text):\n",
    "#     sentences = re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', text, flags=re.U)\n",
    "#     sentences_under512 = []\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "        \n",
    "#         l = len(sentence)\n",
    "        \n",
    "#         if l > 512:\n",
    "#             num = int(l/512)+1\n",
    "#             sentences_under512 += textwrap.wrap(sentence, int(l/num))\n",
    "#         else:\n",
    "#             sentences_under512.append(sentence)\n",
    "            \n",
    "#     return sentences_under512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_len(seqs, to_len, padding=0):\n",
    "    paddeds = []\n",
    "    for seq in seqs:\n",
    "        paddeds.append(\n",
    "            seq[:to_len] + [padding] * max(0, to_len - len(seq))\n",
    "        )\n",
    "\n",
    "    return paddeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        instance = {\n",
    "            'id': sample['ID'],\n",
    "            'token': sample['token_id'],\n",
    "        }\n",
    "        return instance\n",
    "    \n",
    "    def collate_fn(self, samples):\n",
    "        batch = {}\n",
    "        \n",
    "        for key in ['id']:\n",
    "            if any(key not in sample for sample in samples):\n",
    "                continue\n",
    "            batch[key] = [sample[key] for sample in samples]\n",
    "            \n",
    "        for key in ['token']:\n",
    "            if any(key not in sample for sample in samples):\n",
    "                continue\n",
    "                \n",
    "            padded = pad_to_len(\n",
    "                [sample[key] for sample in samples], 512, 0\n",
    "            )\n",
    "            batch[key] = torch.tensor(padded)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(s):    \n",
    "    if s == '[]':\n",
    "        return []\n",
    "    return [str(i.replace(' ', '')[1:-1]) for i in s[1:-1].split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_indexes(input_str, search_str):\n",
    "    l1 = []\n",
    "    length = len(input_str)\n",
    "    index = 0\n",
    "    while index < length:\n",
    "        i = input_str.find(search_str, index)\n",
    "        if i == -1:\n",
    "            return l1\n",
    "        l1.append(i)\n",
    "        index = i + 1\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(name_index, sentence_index):\n",
    "    \n",
    "    if name_index == []:\n",
    "        return []\n",
    "    \n",
    "    arr = []\n",
    "    j = 0\n",
    "    l_name = len(name_index)\n",
    "    l_sentence = len(sentence_index)\n",
    "    \n",
    "    while j < l_sentence:\n",
    "        if name_index[0] == sentence_index[j]:\n",
    "            \n",
    "            flag = 1\n",
    "            record = j\n",
    "            \n",
    "            for i in range(l_name):\n",
    "                if name_index[i] != sentence_index[j]:\n",
    "                    flag = 0\n",
    "                    break\n",
    "                j+=1\n",
    "                \n",
    "            if flag:\n",
    "                j-=1\n",
    "                arr+=[i for i in range(record, record+l_name)]\n",
    "\n",
    "        j+=1\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('data/train.csv')\n",
    "train_df  = train_csv.drop(['hyperlink', 'content', 'domain', 'name'], axis=1)\n",
    "\n",
    "# load pretrained bert model \n",
    "tokenizer_chinese = BertTokenizer.from_pretrained(\"bert-base-chinese\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 10.6 ms, total: 14.9 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['length']    = train_csv['article'].apply(lambda x: len(x))\n",
    "train_df['sentences'] = train_csv['article'].apply(lambda x: split_sentence(x))\n",
    "train_df['token_ids'] = train_df['sentences'].apply(lambda x: [preprocess(sentence) for sentence in x])\n",
    "train_df['blacklist'] = train_csv['name'].apply(lambda x: str2list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5023/5023 [00:00<00:00, 298172.71it/s]\n"
     ]
    }
   ],
   "source": [
    "data_collection = []\n",
    "\n",
    "for data in tqdm.tqdm(train_df.values):\n",
    "    news_ID, article, length, sentences, token_ids, blacklist = data\n",
    "    \n",
    "    for token_id, sentence in zip(token_ids, sentences):\n",
    "        sample = {}\n",
    "        sample['ID'] = news_ID\n",
    "        sample['original_article'] = article\n",
    "        sample['length'] = length\n",
    "        sample['sentence'] = sentence\n",
    "        sample['token_id'] = token_id\n",
    "        \n",
    "        data_collection.append(sample)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dataset = Bert_dataset(data_collection)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = bert_dataset,\n",
    "    batch_size = 32,\n",
    "    collate_fn = lambda x: Bert_dataset.collate_fn(bert_dataset, x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11137 349\n"
     ]
    }
   ],
   "source": [
    "print(len(bert_dataset), len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 349/349 [02:06<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "answer = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(tqdm.tqdm(train_loader), 0):\n",
    "        \n",
    "        ## input\n",
    "        b_input_token = batch['token'].to(NRE_trainer.device)\n",
    "        \n",
    "        ## model\n",
    "        output = model(b_input_token)\n",
    "        \n",
    "        ## output indices\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        \n",
    "        ## post precessing\n",
    "        for i, input_token in enumerate(b_input_token.to('cpu').numpy()):\n",
    "            \n",
    "            words  = tokenizer_chinese.convert_ids_to_tokens(input_token)\n",
    "            labels = label_indices[i]\n",
    "            ID     = batch['id'][i]\n",
    "                                \n",
    "            names = []\n",
    "            name_string = ''\n",
    "            \n",
    "            for word, label in zip(words, labels):\n",
    "                if label == 1:\n",
    "                    name_string += word\n",
    "                else:\n",
    "                    if name_string != '':\n",
    "                        names.append(name_string)\n",
    "                        name_string = ''\n",
    "                        \n",
    "            if ID in answer:\n",
    "                arr = answer[ID]\n",
    "                arr+=names\n",
    "            else:\n",
    "                answer[ID] = names\n",
    "                \n",
    "#         if step > 0:\n",
    "#             break\n",
    "                \n",
    "for key in answer:\n",
    "    answer[key] = list(set(answer[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['name'] = train_df.news_ID.apply(lambda x: answer[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_ID</th>\n",
       "      <th>article</th>\n",
       "      <th>length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>blacklist</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>理財基金量化交易追求絕對報酬 有效對抗牛熊市鉅亨網記者 鄭心芸2019/07/05 22:3...</td>\n",
       "      <td>1723</td>\n",
       "      <td>[理財基金量化交易追求絕對報酬 有效對抗牛熊市鉅亨網記者 鄭心芸2019/07/05 22:...</td>\n",
       "      <td>[[101, 4415, 6512, 1825, 7032, 7030, 1265, 769...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[詹姆斯·西蒙斯, 張堯勇, 鄭心芸]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10月13日晚間發生Uber Eats黃姓外送人員職災死亡案件，北市府勞動局認定業者未依職業...</td>\n",
       "      <td>726</td>\n",
       "      <td>[10月13日晚間發生Uber Eats黃姓外送人員職災死亡案件，北市府勞動局認定業者未依職...</td>\n",
       "      <td>[[101, 8108, 3299, 8124, 3189, 3241, 7279, 463...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[賴香伶, 康水順, 黃]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>社會2019.10.08 09:53【法拍有詭4】飯店遭管委會斷水斷電員工怒吼：生計何去何從...</td>\n",
       "      <td>1154</td>\n",
       "      <td>[社會2019.10.08 09:53【法拍有詭4】飯店遭管委會斷水斷電員工怒吼：生計何去何...</td>\n",
       "      <td>[[101, 4852, 3298, 9160, 119, 8108, 119, 8142,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[張慶輝, 林, 李育材, 李日順]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>文章已被刪除 404 or 例外</td>\n",
       "      <td>16</td>\n",
       "      <td>[文章已被刪除 404 or 例外]</td>\n",
       "      <td>[[101, 3152, 4995, 2347, 6158, 1165, 7370, 105...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>例稿名稱：臺灣屏東地方法院公示催告公告發文日期：中華民國108年9月20日發文字號：屏院進家...</td>\n",
       "      <td>671</td>\n",
       "      <td>[例稿名稱：臺灣屏東地方法院公示催告公告發文日期：中華民國108年9月20日發文字號：屏院進...</td>\n",
       "      <td>[[101, 891, 4943, 1399, 4935, 8038, 5637, 4124...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[沈君融, 陳世恒]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>5019</td>\n",
       "      <td>香港特首林鄭月娥4日宣布撤回逃犯條例修訂，示威者斥為「太遲太少」，「一碗水救不了森林大火」，...</td>\n",
       "      <td>529</td>\n",
       "      <td>[香港特首林鄭月娥4日宣布撤回逃犯條例修訂，示威者斥為「太遲太少」，「一碗水救不了森林大火」...</td>\n",
       "      <td>[[101, 7676, 3949, 4294, 7674, 3360, 6972, 329...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[林鄭月娥]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5019</th>\n",
       "      <td>5020</td>\n",
       "      <td>台股台股盤勢【華冠投顧】OTC轉強 小樹走高華冠投顧※來源：華冠投顧2019/07/15 1...</td>\n",
       "      <td>489</td>\n",
       "      <td>[台股台股盤勢【華冠投顧】OTC轉強 小樹走高華冠投顧※來源：華冠投顧2019/07/15 ...</td>\n",
       "      <td>[[101, 1378, 5500, 1378, 5500, 4676, 1248, 523...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[謝宗霖, 川普]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5020</th>\n",
       "      <td>5021</td>\n",
       "      <td>近日教育部在媒體上宣布駁回世新大學社發所的停招申請案，但卻沒同時宣布其他大學類似的申請案，其...</td>\n",
       "      <td>2030</td>\n",
       "      <td>[近日教育部在媒體上宣布駁回世新大學社發所的停招申請案，但卻沒同時宣布其他大學類似的申請案，...</td>\n",
       "      <td>[[101, 6818, 3189, 3136, 5509, 6956, 1762, 205...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[江漢聲, 王英洲, 潘文忠, 朱俊彰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>5022</td>\n",
       "      <td>史上金額最大開發案「台北雙子星」最優申請人「南海團隊」香港商南海發展有限公司、馬來西亞商馬頓...</td>\n",
       "      <td>932</td>\n",
       "      <td>[史上金額最大開發案「台北雙子星」最優申請人「南海團隊」香港商南海發展有限公司、馬來西亞商馬...</td>\n",
       "      <td>[[101, 1380, 677, 7032, 7540, 3297, 1920, 7274...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>5023</td>\n",
       "      <td>期貨股票影/今年上半年 三大股票期貨交易策略最賺錢鉅亨網記者 鄭心芸、李佳泓2019/09/...</td>\n",
       "      <td>311</td>\n",
       "      <td>[期貨股票影/今年上半年 三大股票期貨交易策略最賺錢鉅亨網記者 鄭心芸、李佳泓2019/09...</td>\n",
       "      <td>[[101, 3309, 6515, 5500, 4873, 2512, 120, 791,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[黃維本, 鄭心芸, 李佳泓]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5023 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_ID                                            article  length  \\\n",
       "0           1  理財基金量化交易追求絕對報酬 有效對抗牛熊市鉅亨網記者 鄭心芸2019/07/05 22:3...    1723   \n",
       "1           2  10月13日晚間發生Uber Eats黃姓外送人員職災死亡案件，北市府勞動局認定業者未依職業...     726   \n",
       "2           3  社會2019.10.08 09:53【法拍有詭4】飯店遭管委會斷水斷電員工怒吼：生計何去何從...    1154   \n",
       "3           4                                   文章已被刪除 404 or 例外      16   \n",
       "4           5  例稿名稱：臺灣屏東地方法院公示催告公告發文日期：中華民國108年9月20日發文字號：屏院進家...     671   \n",
       "...       ...                                                ...     ...   \n",
       "5018     5019  香港特首林鄭月娥4日宣布撤回逃犯條例修訂，示威者斥為「太遲太少」，「一碗水救不了森林大火」，...     529   \n",
       "5019     5020  台股台股盤勢【華冠投顧】OTC轉強 小樹走高華冠投顧※來源：華冠投顧2019/07/15 1...     489   \n",
       "5020     5021  近日教育部在媒體上宣布駁回世新大學社發所的停招申請案，但卻沒同時宣布其他大學類似的申請案，其...    2030   \n",
       "5021     5022  史上金額最大開發案「台北雙子星」最優申請人「南海團隊」香港商南海發展有限公司、馬來西亞商馬頓...     932   \n",
       "5022     5023  期貨股票影/今年上半年 三大股票期貨交易策略最賺錢鉅亨網記者 鄭心芸、李佳泓2019/09/...     311   \n",
       "\n",
       "                                              sentences  \\\n",
       "0     [理財基金量化交易追求絕對報酬 有效對抗牛熊市鉅亨網記者 鄭心芸2019/07/05 22:...   \n",
       "1     [10月13日晚間發生Uber Eats黃姓外送人員職災死亡案件，北市府勞動局認定業者未依職...   \n",
       "2     [社會2019.10.08 09:53【法拍有詭4】飯店遭管委會斷水斷電員工怒吼：生計何去何...   \n",
       "3                                    [文章已被刪除 404 or 例外]   \n",
       "4     [例稿名稱：臺灣屏東地方法院公示催告公告發文日期：中華民國108年9月20日發文字號：屏院進...   \n",
       "...                                                 ...   \n",
       "5018  [香港特首林鄭月娥4日宣布撤回逃犯條例修訂，示威者斥為「太遲太少」，「一碗水救不了森林大火」...   \n",
       "5019  [台股台股盤勢【華冠投顧】OTC轉強 小樹走高華冠投顧※來源：華冠投顧2019/07/15 ...   \n",
       "5020  [近日教育部在媒體上宣布駁回世新大學社發所的停招申請案，但卻沒同時宣布其他大學類似的申請案，...   \n",
       "5021  [史上金額最大開發案「台北雙子星」最優申請人「南海團隊」香港商南海發展有限公司、馬來西亞商馬...   \n",
       "5022  [期貨股票影/今年上半年 三大股票期貨交易策略最賺錢鉅亨網記者 鄭心芸、李佳泓2019/09...   \n",
       "\n",
       "                                              token_ids blacklist  \\\n",
       "0     [[101, 4415, 6512, 1825, 7032, 7030, 1265, 769...        []   \n",
       "1     [[101, 8108, 3299, 8124, 3189, 3241, 7279, 463...        []   \n",
       "2     [[101, 4852, 3298, 9160, 119, 8108, 119, 8142,...        []   \n",
       "3     [[101, 3152, 4995, 2347, 6158, 1165, 7370, 105...        []   \n",
       "4     [[101, 891, 4943, 1399, 4935, 8038, 5637, 4124...        []   \n",
       "...                                                 ...       ...   \n",
       "5018  [[101, 7676, 3949, 4294, 7674, 3360, 6972, 329...        []   \n",
       "5019  [[101, 1378, 5500, 1378, 5500, 4676, 1248, 523...        []   \n",
       "5020  [[101, 6818, 3189, 3136, 5509, 6956, 1762, 205...        []   \n",
       "5021  [[101, 1380, 677, 7032, 7540, 3297, 1920, 7274...        []   \n",
       "5022  [[101, 3309, 6515, 5500, 4873, 2512, 120, 791,...        []   \n",
       "\n",
       "                      name  \n",
       "0      [詹姆斯·西蒙斯, 張堯勇, 鄭心芸]  \n",
       "1            [賴香伶, 康水順, 黃]  \n",
       "2       [張慶輝, 林, 李育材, 李日順]  \n",
       "3                       []  \n",
       "4               [沈君融, 陳世恒]  \n",
       "...                    ...  \n",
       "5018                [林鄭月娥]  \n",
       "5019             [謝宗霖, 川普]  \n",
       "5020  [江漢聲, 王英洲, 潘文忠, 朱俊彰]  \n",
       "5021                    []  \n",
       "5022       [黃維本, 鄭心芸, 李佳泓]  \n",
       "\n",
       "[5023 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
