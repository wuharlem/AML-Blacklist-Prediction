{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tool\n",
    "import pickle\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "## Bert\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "## module\n",
    "from module.ner_trainer import NRE_Trainer\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenized_sentence = tokenizer_chinese.encode(text)\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(text):\n",
    "    sentences = []\n",
    "    if len(text) < 500:\n",
    "        sentences.append(text)\n",
    "        \n",
    "    elif len(text) > 500 and len(text) <= 1000:\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    elif len(text) > 1000 and len(text) <= 1500:\n",
    "        mid = int(len(text)/2)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[mid-250:mid+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    elif len(text) > 1500 and len(text) <= 2000:\n",
    "        point_1 = int(len(text)*0.25)\n",
    "        point_2 = int(len(text)*0.75)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[point_1-250:point_1+250])\n",
    "        sentences.append(text[point_2-250:point_2+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    else:\n",
    "        mid = int(len(text)/2)\n",
    "        point_1 = int(len(text)*0.25)\n",
    "        point_2 = int(len(text)*0.75)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[point_1-250:point_1+250])\n",
    "        sentences.append(text[mid-250:mid+250])\n",
    "        sentences.append(text[point_2-250:point_2+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_len(seqs, to_len, padding=0):\n",
    "    paddeds = []\n",
    "    for seq in seqs:\n",
    "        paddeds.append(\n",
    "            seq[:to_len] + [padding] * max(0, to_len - len(seq))\n",
    "        )\n",
    "    return paddeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        instance = {\n",
    "            'id': sample['ID'],\n",
    "            'words': sample['token_id'],\n",
    "            'tag' : sample['label'],\n",
    "            'segment': [0]*len(sample['token_id']),\n",
    "#             'mask': [1]*(len(sample['token_id']))\n",
    "            'mask': sample['mask']\n",
    "        }\n",
    "        return instance\n",
    "    \n",
    "    def collate_fn(self, samples):\n",
    "        batch = {}\n",
    "        for key in ['id']:\n",
    "            if any(key not in sample for sample in samples):\n",
    "                continue\n",
    "            batch[key] = [sample[key] for sample in samples]\n",
    "            \n",
    "        for key in ['words', 'tag', 'segment', 'mask']:\n",
    "            if any(key not in sample for sample in samples):\n",
    "                continue\n",
    "            to_len = max([len(sample[key]) for sample in samples])\n",
    "            if key =='tag':\n",
    "                pad_logit = 2\n",
    "            else:\n",
    "                pad_logit = 0\n",
    "            padded = pad_to_len(\n",
    "                [sample[key] for sample in samples], 512, pad_logit\n",
    "            )\n",
    "            batch[key] = torch.tensor(padded).long()\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(s):    \n",
    "    if s == '[]':\n",
    "        return []\n",
    "    return [str(i.replace(' ', '')[1:-1]) for i in s[1:-1].split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(name_index, sentence_index):\n",
    "    \n",
    "    if name_index == []:\n",
    "        return []\n",
    "    \n",
    "    arr = []\n",
    "    j = 0\n",
    "    l_name = len(name_index)\n",
    "    l_sentence = len(sentence_index)\n",
    "    \n",
    "    while j < l_sentence:\n",
    "        if name_index[0] == sentence_index[j]:\n",
    "            \n",
    "            flag = 1\n",
    "            record = j\n",
    "            \n",
    "            for i in range(l_name):\n",
    "                if name_index[i] != sentence_index[j]:\n",
    "                    flag = 0\n",
    "                    break\n",
    "                j+=1\n",
    "                \n",
    "            if flag:\n",
    "                j-=1\n",
    "                arr+=[i for i in range(record, record+l_name)]\n",
    "        j+=1\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('data/train.csv')\n",
    "train_df  = train_csv.drop(['hyperlink', 'content', 'domain', 'name'], axis=1)\n",
    "\n",
    "# load pretrained bert model \n",
    "tokenizer_chinese = BertTokenizer.from_pretrained(\"bert-base-chinese\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df['length']    = train_csv['article'].apply(lambda x: len(x))\n",
    "train_df['sentences'] = train_csv['article'].apply(lambda x: split_sentence(x))\n",
    "train_df['token_ids'] = train_df['sentences'].apply(lambda x: [preprocess(sentence) for sentence in x])\n",
    "train_df['blacklist'] = train_csv['name'].apply(lambda x: str2list(x))\n",
    "train_df['names']     = pd.read_csv('data/name.csv')['0'].apply(lambda x: str2list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete some useless data\n",
    "train_df = train_df[train_df.article != '文章已被刪除 404 or 例外']\n",
    "train_df = train_df[(train_df['length']) <= 2551]\n",
    "train_df = train_df[(train_df['length']) >= 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json_type = {}\n",
    "data_collection  = []\n",
    "\n",
    "for data in tqdm.tqdm(train_df.values):\n",
    "    news_ID, article, length, sentences, token_ids, blacklist, names = data\n",
    "    \n",
    "    for token_id, sentence in zip(token_ids, sentences):\n",
    "        sample = {}\n",
    "        sample['ID'] = news_ID\n",
    "        sample['original_article'] = article\n",
    "        sample['length'] = length\n",
    "        sample['sentence'] = sentence\n",
    "        sample['token_id'] = token_id\n",
    "        sample['blacklist'] = blacklist\n",
    "        sample['names'] = names \n",
    "        \n",
    "        ## get label\n",
    "        one_hot = torch.zeros(len(token_id)).tolist()\n",
    "        one_hot2 = torch.zeros(len(token_id)).tolist()\n",
    "        \n",
    "        position, position2 = [], []\n",
    "        \n",
    "        for black_name in blacklist:\n",
    "            black_name_ids = tokenizer_chinese.encode(black_name)[1:-1]\n",
    "            position+=get_index(black_name_ids, token_id)\n",
    "            \n",
    "        for i in position:\n",
    "            one_hot[i] = 1\n",
    "            \n",
    "        for name in names:\n",
    "            name_ids = tokenizer_chinese.encode(name)[1:-1]\n",
    "            position2+=get_index(name_ids, token_id)\n",
    "            \n",
    "        for i in position2:\n",
    "            one_hot2[i] = 1\n",
    "        \n",
    "        sample['label'] = one_hot\n",
    "        sample['mask']  = one_hot2\n",
    "        \n",
    "    if news_ID in answer_json_type:\n",
    "        pass\n",
    "    else:\n",
    "        answer_json_type[news_ID] = {'names': names, 'blacklist': blacklist}\n",
    "        \n",
    "    data_collection.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "bert_dataset = Bert_dataset(data_collection)\n",
    "\n",
    "train_dataset, test_dataset = random_split(bert_dataset,\n",
    "                                           [int(len(bert_dataset)*0.8), \n",
    "                                            len(bert_dataset)-int(len(bert_dataset)*0.8)])\n",
    "\n",
    "valid_dataset, test_dataset = random_split(test_dataset, \n",
    "                                           [int(len(test_dataset)*0.5), \n",
    "                                            len(test_dataset)-int(len(test_dataset)*0.5)])\n",
    "\n",
    "## Hyperparameter\n",
    "n_train = len(train_dataset)\n",
    "n_valid = len(valid_dataset)\n",
    "n_test  = len(test_dataset)\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([1 for i in train_dataset if 1 in i['tag']]), n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([1 for i in valid_dataset if 1 in i['tag']]), n_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([1 for i in test_dataset if 1 in i['tag']]), n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    collate_fn = lambda x: Bert_dataset.collate_fn(train_dataset, x)\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = lambda x: Bert_dataset.collate_fn(valid_dataset, x)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = lambda x: Bert_dataset.collate_fn(test_dataset, x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-chinese\",\n",
    "    num_labels = 3,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda\n"
     ]
    }
   ],
   "source": [
    "trainer = NRE_Trainer(model, train_loader, valid_loader)\n",
    "trainer.tag2idx = { 'O': 0, 'blacklist': 1, 'PAD': 2}\n",
    "trainer.tag_values = ['O', 'blacklist', 'PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.load_state_dict(torch.load('params/best-model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda classification acc:  0.9712 validation loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "acc, total_loss = trainer.evaluation(test=False)\n",
    "print(f\"device: {trainer.device} classification acc: {acc: .4f} validation loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:08<00:00,  3.59it/s]\n",
      "100%|██████████| 58/58 [00:05<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " time: 2m 15s | epoch:0 train loss: 0.0000 | validation loss: 0.0000 acc: 0.9712"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 152/459 [00:42<01:25,  3.58it/s]"
     ]
    }
   ],
   "source": [
    "trainer.training_process(early_stopping = True, \n",
    "                         n_iter_no_change = 5, \n",
    "                         max_epoch = 10, \n",
    "                         save_params = True, \n",
    "                         verbose = True, \n",
    "                         learning_rate = 3e-5, \n",
    "                         save_paths='model-best-fine-tune.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.load_state_dict(torch.load('params/model-best-fine-tune.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:05<00:00,  9.74it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_answer = {}\n",
    "answer = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, batch in enumerate(tqdm.tqdm(test_loader)):\n",
    "        b_input_token  = batch['words'].to(trainer.device)\n",
    "        output = model(b_input_token)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        \n",
    "        for i, input_token in enumerate(b_input_token.to('cpu').numpy()):\n",
    "            \n",
    "            words  = tokenizer_chinese.convert_ids_to_tokens(input_token)\n",
    "            labels = label_indices[i]\n",
    "            ID     = batch['id'][i]\n",
    "                                \n",
    "            names = []\n",
    "            name_string = ''\n",
    "            \n",
    "            for word, label in zip(words, labels):\n",
    "                if label == 1:\n",
    "                    name_string += word\n",
    "                else:\n",
    "                    if name_string != '':\n",
    "                        names.append(name_string)\n",
    "                        name_string = ''\n",
    "            \n",
    "            if ID in answer:\n",
    "                arr = answer[ID]\n",
    "                arr+=names\n",
    "            else:\n",
    "                answer[ID] = names\n",
    "\n",
    "\n",
    "for key in answer:\n",
    "    answer[key] = list(set(answer[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(truth, predict):\n",
    "    if truth == [] and predict != []:\n",
    "        return 0\n",
    "    \n",
    "    if truth != [] and predict == []:\n",
    "        return 0\n",
    "    \n",
    "    if truth == [] and predict == []:\n",
    "        return 1\n",
    "    \n",
    "    recall = len([i for i in truth if i in predict])/len(truth)\n",
    "    precision = len([i for i in predict if i in truth])/len(predict)\n",
    "    \n",
    "    try:\n",
    "        return 2/((1/recall)+(1/precision))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448.2265512265513\n",
      "0.9765284340447741\n"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "\n",
    "for ID in answer:\n",
    "    blacklist = answer_json_type[ID]['blacklist']\n",
    "    \n",
    "    total_score+=score(answer[ID], blacklist)\n",
    "    \n",
    "#     print(ID, 'Answer:',answer[ID], \n",
    "#           '黑名單:', answer_json_type[ID]['blacklist'],\n",
    "# #           'Names:', answer_json_type[ID]['names'],\n",
    "#           'Score:', score(answer[ID], blacklist)\n",
    "#          )\n",
    "\n",
    "print(total_score)\n",
    "print(total_score/len(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
