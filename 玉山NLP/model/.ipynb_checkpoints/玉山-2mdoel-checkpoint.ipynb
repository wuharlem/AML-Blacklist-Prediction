{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tool\n",
    "import pickle\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "## plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "## Bert\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "## module\n",
    "from module.ner_trainer import NRE_Trainer\n",
    "\n",
    "## torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-chinese\",\n",
    "    num_labels = 3,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "NRE_trainer = NRE_Trainer(model, None, None)\n",
    "NRE_trainer.model.load_state_dict(torch.load('params/best-model-test.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測名字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenized_sentence = tokenizer_chinese.encode(text)\n",
    "    input_ids = tokenized_sentence\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(text):\n",
    "    sentences = []\n",
    "    if len(text) < 500:\n",
    "        sentences.append(text)\n",
    "        \n",
    "    elif len(text) > 500 and len(text) <= 1000:\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    elif len(text) > 1000 and len(text) <= 1500:\n",
    "        mid = int(len(text)/2)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[mid-250:mid+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    elif len(text) > 1500 and len(text) <= 2000:\n",
    "        point_1 = int(len(text)*0.25)\n",
    "        point_2 = int(len(text)*0.75)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[point_1-250:point_1+250])\n",
    "        sentences.append(text[point_2-250:point_2+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    else:\n",
    "        mid = int(len(text)/2)\n",
    "        point_1 = int(len(text)*0.25)\n",
    "        point_2 = int(len(text)*0.75)\n",
    "        sentences.append(text[:500])\n",
    "        sentences.append(text[point_1-250:point_1+250])\n",
    "        sentences.append(text[mid-250:mid+250])\n",
    "        sentences.append(text[point_2-250:point_2+250])\n",
    "        sentences.append(text[-500:])\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_sentence(text):\n",
    "#     sentences = re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', text, flags=re.U)\n",
    "#     sentences_under512 = []\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "        \n",
    "#         l = len(sentence)\n",
    "        \n",
    "#         if l > 512:\n",
    "#             num = int(l/512)+1\n",
    "#             sentences_under512 += textwrap.wrap(sentence, int(l/num))\n",
    "#         else:\n",
    "#             sentences_under512.append(sentence)\n",
    "            \n",
    "#     return sentences_under512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_len(seqs, to_len, padding=0):\n",
    "    paddeds = []\n",
    "    for seq in seqs:\n",
    "        paddeds.append(\n",
    "            seq[:to_len] + [padding] * max(0, to_len - len(seq))\n",
    "        )\n",
    "\n",
    "    return paddeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        instance = {\n",
    "            'id': sample['ID'],\n",
    "            'token': sample['token_id'],\n",
    "        }\n",
    "        return instance\n",
    "    \n",
    "    def collate_fn(self, samples):\n",
    "        batch = {}\n",
    "        \n",
    "        for key in ['id']:\n",
    "            if any(key not in sample for sample in samples):\n",
    "                continue\n",
    "            batch[key] = [sample[key] for sample in samples]\n",
    "            \n",
    "        for key in ['token']:\n",
    "            if any(key not in sample for sample in samples):\n",
    "                continue\n",
    "                \n",
    "            padded = pad_to_len(\n",
    "                [sample[key] for sample in samples], 512, 0\n",
    "            )\n",
    "            batch[key] = torch.tensor(padded)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(s):    \n",
    "    if s == '[]':\n",
    "        return []\n",
    "    return [str(i.replace(' ', '')[1:-1]) for i in s[1:-1].split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_indexes(input_str, search_str):\n",
    "    l1 = []\n",
    "    length = len(input_str)\n",
    "    index = 0\n",
    "    while index < length:\n",
    "        i = input_str.find(search_str, index)\n",
    "        if i == -1:\n",
    "            return l1\n",
    "        l1.append(i)\n",
    "        index = i + 1\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(name_index, sentence_index):\n",
    "    \n",
    "    if name_index == []:\n",
    "        return []\n",
    "    \n",
    "    arr = []\n",
    "    j = 0\n",
    "    l_name = len(name_index)\n",
    "    l_sentence = len(sentence_index)\n",
    "    \n",
    "    while j < l_sentence:\n",
    "        if name_index[0] == sentence_index[j]:\n",
    "            \n",
    "            flag = 1\n",
    "            record = j\n",
    "            \n",
    "            for i in range(l_name):\n",
    "                if name_index[i] != sentence_index[j]:\n",
    "                    flag = 0\n",
    "                    break\n",
    "                j+=1\n",
    "                \n",
    "            if flag:\n",
    "                j-=1\n",
    "                arr+=[i for i in range(record, record+l_name)]\n",
    "\n",
    "        j+=1\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('data/train.csv')\n",
    "train_df  = train_csv.drop(['hyperlink', 'content', 'domain', 'name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained bert model \n",
    "tokenizer_chinese = BertTokenizer.from_pretrained(\"bert-base-chinese\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 23.8 ms, total: 14.9 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['length']    = train_csv['article'].apply(lambda x: len(x))\n",
    "train_df['sentences'] = train_csv['article'].apply(lambda x: split_sentence(x))\n",
    "train_df['token_ids'] = train_df['sentences'].apply(lambda x: [preprocess(sentence) for sentence in x])\n",
    "train_df['blacklist'] = train_csv['name'].apply(lambda x: str2list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5023/5023 [00:00<00:00, 326084.43it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_data = []\n",
    "\n",
    "for data in tqdm.tqdm(train_df.values):\n",
    "    news_ID, article, length, sentences, token_ids, blacklist = data\n",
    "    \n",
    "    sample = {}\n",
    "    \n",
    "    for token_id, sentence in zip(token_ids, sentences):\n",
    "        sample['ID'] = news_ID\n",
    "        sample['original_article'] = article\n",
    "        sample['length'] = length\n",
    "        sample['sentence'] = sentence\n",
    "        sample['token_id'] = token_id\n",
    "        \n",
    "        raw_data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dataset = Bert_dataset(raw_data)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = bert_dataset,\n",
    "    batch_size = 32,\n",
    "    collate_fn = lambda x: Bert_dataset.collate_fn(bert_dataset, x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11137 349\n"
     ]
    }
   ],
   "source": [
    "print(len(bert_dataset), len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 284/349 [01:43<00:23,  2.73it/s]"
     ]
    }
   ],
   "source": [
    "answer = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(tqdm.tqdm(train_loader), 0):\n",
    "        b_input_token = batch['token'].to(NRE_trainer.device)\n",
    "        output = model(b_input_token)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        \n",
    "        for i, input_token in enumerate(b_input_token.to('cpu').numpy()):\n",
    "            \n",
    "            words  = tokenizer_chinese.convert_ids_to_tokens(input_token)\n",
    "            labels = label_indices[i]\n",
    "            ID     = batch['id'][i]\n",
    "                                \n",
    "            names = []\n",
    "            name_string = ''\n",
    "            \n",
    "            for word, label in zip(words, labels):\n",
    "                if label == 1:\n",
    "                    name_string += word\n",
    "                else:\n",
    "                    if name_string != '':\n",
    "                        names.append(name_string)\n",
    "                        name_string = ''\n",
    "                        \n",
    "            if ID in answer:\n",
    "                arr = answer[ID]\n",
    "                arr+=names\n",
    "            else:\n",
    "                answer[ID] = names\n",
    "                \n",
    "for key in answer:\n",
    "    answer[key] = list(set(answer[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['name'] = train_df.news_ID.apply(lambda x: answer[x-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
